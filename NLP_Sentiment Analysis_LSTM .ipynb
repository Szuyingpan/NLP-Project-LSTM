{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca6bfe6",
   "metadata": {},
   "source": [
    "## Natual Language Processing for Sentiment Analysis by using Amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Numeric and Data handling -\n",
    "# \n",
    "# PanDas\n",
    "# NumPy\n",
    "# bz2\n",
    "# \n",
    "# For environmental uses -\n",
    "# os\n",
    "# \n",
    "# For string processing -\n",
    "# re\n",
    "# \n",
    "# For Preprocessing -\n",
    "# \n",
    "# Tensorflow Preprocessing\n",
    "# Tokenizer\n",
    "# t2w sequence\n",
    "# pad sequences\n",
    "# \n",
    "# For Modelling -\n",
    "# Tensorflow\n",
    "# \n",
    "# For metrics -\n",
    "# Confusion metrix\n",
    "# F1 Score\n",
    "# ROC AUC Score\n",
    "# Accuracy Scores\n",
    "# \n",
    "# For Plotting -\n",
    "# Matplotlib\n",
    "# Seaborn\n",
    "\n",
    "# ## 1. Import Library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1c33b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65bd8d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Basic Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# NLTK libraries\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Metric Libraries \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ML\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#!pip install tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "#from gensim import corpora as corpora\n",
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from tensorflow.keras.layers import Dense,LSTM,SpatialDropout1D,Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785f966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec0358f0",
   "metadata": {},
   "source": [
    "## 2. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import bz2\n",
    "\n",
    "# Step 1: Unzip the .zip file to extract the .bz2 file\n",
    "zip_file_path = '/Users/szuyingpan/Desktop/NLP/CW1/train.ft.txt.bz.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')  # Extract all files in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zip_file_path = '/Users/szuyingpan/Desktop/NLP/CW1/test.ft.txt.bz.zip'  # Make sure this path is correct\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(zip_file_path):\n",
    "    print(\"The file exists.\")\n",
    "else:\n",
    "    print(\"The file does not exist. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea216e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    \"\"\"Reads a text file into a list of strings.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "# the decompressed files \n",
    "train_data = read_text_file('train.ft.txt')\n",
    "test_data = read_text_file('test.ft.txt')\n",
    "\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ecd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preview_file(file_path, num_lines=5):\n",
    "    \"\"\"Reads a file and prints a preview of the first few lines.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i >= num_lines: break\n",
    "            print(line.strip())  # Print line with trailing newline stripped\n",
    "\n",
    "# Preview the first few lines of the train and test files\n",
    "read_and_preview_file('train.ft.txt')\n",
    "read_and_preview_file('test.ft.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1677b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Read the archived file line by line, and add it to the list\n",
    "for line in bz2.open(\"train.ft.txt.bz\", \"rt\", encoding=\"utf8\"):\n",
    "    # label 1 is negative and label 2 is positive\n",
    "    label = 1 if line.startswith(\"__label__1\") else 2\n",
    "    text = line[10:].strip()  # Remove the label and any leading/trailing whitespace\n",
    "\n",
    "    localResult = {\n",
    "        \"label\": label,\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "    data.append(localResult)\n",
    "\n",
    "train_df = pd.DataFrame(data)\n",
    "#df = df.reset_index().rename(columns= {\"index\": \"Id\"})\n",
    "train_df = train_df.rename(columns= {\"text\": \"review\"})\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = []\n",
    "\n",
    "# Read the archived file line by line, and add it to the list\n",
    "for line in bz2.open(\"test.ft.txt.bz\", \"rt\", encoding=\"utf8\"):\n",
    "    # label 1 is negative and label 2 is positive\n",
    "    label = 1 if line.startswith(\"__label__1\") else 2\n",
    "    text = line[10:].strip()  # Remove the label and any leading/trailing whitespace\n",
    "\n",
    "    localResult = {\n",
    "        \"label\": label,\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "    data2.append(localResult)\n",
    "\n",
    "test_df = pd.DataFrame(data)\n",
    "test_df = test_df.rename(columns= {\"text\": \"review\"})\n",
    "test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc90186",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"the shape of the data\", train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef2f9b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"unique labels\", train_df[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071df90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valueCounts = train_df[\"label\"].value_counts().sort_index()\n",
    "print(valueCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026e2ba",
   "metadata": {},
   "source": [
    "## 3. Structural cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067d57e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "train_df = train_df.drop_duplicates()\n",
    "\n",
    "# Handle missing values - 'review' is main column\n",
    "train_df = train_df.dropna(subset=['review'])  # Remove rows where 'review' is missing\n",
    "print(train_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e81568",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the shape of the data\", train_df.shape)\n",
    "\n",
    "# There is no mssing or duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae3e3b",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24c60e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Count of good and bad reviews\n",
    "count=train_df['label'].value_counts()\n",
    "print('Total Counts of both sets'.format(),count)\n",
    "\n",
    "print(\"==============\")\n",
    "#Creating a function to plot the counts using matplotlib\n",
    "def plot_counts(count_good,count_bad):\n",
    "    plt.rcParams['figure.figsize']=(6,6)\n",
    "    plt.bar(0,count_good,width=0.6,label='Positive Reviews',color='Green')\n",
    "    plt.legend()\n",
    "    plt.bar(2,count_bad,width=0.6,label='Negative Reviews',color='Red')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Count of Reviews')\n",
    "    plt.xlabel('Types of Reviews')\n",
    "    plt.show()\n",
    "    \n",
    "count_good=train_df[train_df['label']== 2]\n",
    "count_bad=train_df[train_df['label']== 1]\n",
    "plot_counts(len(count_good),len(count_bad))\n",
    "\n",
    "# We can see that two classes are equal in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3977b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews = train_df[train_df['label'] == 2]['review']\n",
    "bad_reviews = train_df[train_df['label'] == 1]['review']\n",
    "print(good_reviews[:10])\n",
    "print(bad_reviews[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb207f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Analyse the count of words in each segment- both positive and negative reviews\n",
    "\n",
    "#Function for checking word length\n",
    "def cal_len(data):\n",
    "    return len(data)\n",
    "\n",
    "#Create generic plotter with Seaborn\n",
    "def plot_count(count_ones,count_zeros,title_1,title_2,subtitle):\n",
    "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
    "    sns.distplot(count_zeros,ax=ax1,color='Blue')\n",
    "    ax1.set_title(title_1)\n",
    "    sns.distplot(count_ones,ax=ax2,color='Red')\n",
    "    ax2.set_title(title_2)\n",
    "    fig.suptitle(subtitle)\n",
    "    plt.show()    \n",
    "\n",
    "count_good_words = good_reviews.str.split().apply(lambda z:cal_len(z))\n",
    "count_bad_words = bad_reviews.str.split().apply(lambda z:cal_len(z))\n",
    "print(\"Positive Review Words:\" + str(count_good_words))\n",
    "print(\"Negative Review Words:\" + str(count_bad_words))\n",
    "plot_count(count_good_words,count_bad_words,\"Positive Review\",\"Negative Review\",\"Reviews Word Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85fe0da",
   "metadata": {},
   "source": [
    "Count Punctuations/Stopwords/Codes and other semantic datatypes\n",
    "\n",
    "Punctuation marks can convey significant information about sentence structure and tone, which might be crucial for certain NLP tasks like sentiment analysis or natural language understanding. We will be using the \"generic_plotter\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_good_punctuations=count_good['review'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n",
    "count_bad_punctuations=count_bad['review'].apply(lambda z:len([c for c in str(z) if c in string.punctuation]))\n",
    "plot_count(count_good_punctuations,count_bad_punctuations,\"Positive Review Punctuations\",\"Negative Review Punctuations\",\"Reviews Word Punctuation Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b8914",
   "metadata": {},
   "source": [
    "Punctuation marks can convey significant information about sentence structure and tone, which might be crucial for certain NLP tasks like sentiment analysis or natural language understanding.\n",
    "Removal: In many NLP tasks, especially those focused on understanding the general content or topic of the text (like topic modeling or keyword extraction), punctuation might not add useful information and can be removed to reduce the complexity of the text data.\n",
    "Preservation: In tasks like text generation, machine translation, or emotion detection, preserving punctuation can be critical as it affects readability and the conveyed emotions or nuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bc330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse Stopwords\n",
    "# Stopwords are common words (such as \"the\", \"is\", \"in\") that are usually filtered out \n",
    "# in the preprocessing phase because they occur frequently and are believed to carry \n",
    "# little meaningful information about the content of the text.\n",
    "\n",
    "def plot_count_1(count_ones,count_zeros,title_1,title_2,subtitle):\n",
    "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n",
    "    sns.distplot(count_zeros,ax=ax1,color='Blue')\n",
    "    ax1.set_title(title_1)\n",
    "    sns.distplot(count_ones,ax=ax2,color='Orange')\n",
    "    ax2.set_title(title_2)\n",
    "    fig.suptitle(subtitle)\n",
    "    plt.show()    \n",
    "\n",
    "stops=set(stopwords.words('english'))\n",
    "count_good_stops=count_good['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
    "count_bad_stops=count_bad['review'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n",
    "plot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews Stopwords\",\"Negative Reviews Stopwords\",\"Reviews Stopwords Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd794e",
   "metadata": {},
   "source": [
    "Removing stopwords is to reduce the dimensionality of the text data, which can improve the performance of NLP models by focusing on more informative words. \n",
    "However, in certain NLP tasks, stopwords can provide important context and should be preserved. For instance, in phrase-based sentiment analysis (\"not good\" versus \"good\"), stopwords like \"not\" drastically change the meaning. Similarly, in language modeling and machine translation, stopwords are crucial for generating syntactically correct sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1dc5a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def display_simple_cloud(data, color='black', max_words=20000):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    wc = WordCloud(stopwords=STOPWORDS, \n",
    "                   background_color=\"white\", \n",
    "                   contour_width=2, \n",
    "                   contour_color=color,\n",
    "                   max_words=max_words,  # Reduced number of words\n",
    "                   width=800,            # Standard width\n",
    "                   height=400)           # Standard height\n",
    "    wc.generate(' '.join(data[:20000]))  # Generate from the first 20000 reviews (as an example)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "display_simple_cloud(good_reviews.tolist(), 'red')  # Convert DataFrame column to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8843b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_simple_cloud(bad_reviews.tolist(), 'blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755a8ba",
   "metadata": {},
   "source": [
    "Note: Our task is sentiment analysis which is context or syntext-based tasks, maintainig punctuation and handling stoptwords can be essential to preserve the original meaning and structure of the text. For example, in phrase-based sentiment analysis (\"not good\" versus \"good\"), stopwords like \"not\" drastically change the meaning. Similarly, in language modeling and machine translation, stopwords are crucial for generating syntactically correct sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87c2dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Removing punctuation? visualise good reviews \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Rreviews is a list of review texts\n",
    "reviews = good_reviews\n",
    "\n",
    "# Combine all reviews into one large string\n",
    "all_reviews = \" \".join(reviews)\n",
    "\n",
    "# Count all punctuation marks in the reviews\n",
    "punctuation_counts = Counter(c for c in all_reviews if c in string.punctuation)\n",
    "\n",
    "# Visualize the counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(punctuation_counts.keys(), punctuation_counts.values())\n",
    "plt.title('Frequency of Punctuation Marks in Reviews')\n",
    "plt.xlabel('Punctuation Mark')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e6907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise bad reviews\n",
    "\n",
    "# Rreviews is a list of review texts\n",
    "reviews = bad_reviews\n",
    "\n",
    "# Combine all reviews into one large string\n",
    "all_reviews = \" \".join(reviews)\n",
    "\n",
    "# Count all punctuation marks in the reviews\n",
    "punctuation_counts = Counter(c for c in all_reviews if c in string.punctuation)\n",
    "\n",
    "# Visualize the counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(punctuation_counts.keys(), punctuation_counts.values())\n",
    "plt.title('Frequency of Punctuation Marks in Reviews')\n",
    "plt.xlabel('Punctuation Mark')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ddd52c",
   "metadata": {},
   "source": [
    "From the graph, we can see that punctuation marks with emotional connotations, such as \"!\" and \"?\", occur less frequently in the text. In this case, we will remove them in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86a47d",
   "metadata": {},
   "source": [
    "## 4.1 Text Cleaning and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e31783",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import re\n",
    "\n",
    "# Removes Punctuations\n",
    "def remove_punctuations(text):\n",
    "    punct_tag = re.compile(r'[^\\w\\s]')\n",
    "    text = punct_tag.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Removes HTML syntaxes\n",
    "def remove_html(text):\n",
    "    html_tag = re.compile(r'<.*?>')\n",
    "    text = html_tag.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Removes URL data\n",
    "def remove_url(text):\n",
    "    url_clean = re.compile(r\"https://\\S+|www\\.\\S+\")\n",
    "    text = url_clean.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Removes Emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_clean = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_clean.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# Applying all functions to the 'review' column\n",
    "train_df['review'] = train_df['review'].apply(lambda z: remove_punctuations(z))\n",
    "train_df['review'] = train_df['review'].apply(lambda z: remove_html(z))\n",
    "train_df['review'] = train_df['review'].apply(lambda z: remove_url(z))\n",
    "train_df['review'] = train_df['review'].apply(lambda z: remove_emoji(z))\n",
    "train_df['review'] = train_df['review'].apply(lambda z: to_lowercase(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6341efe",
   "metadata": {},
   "source": [
    "Since our task is sentiment analysis, we can remove unnecessary information such as punctuation, urls and convert text to lowercase to ensure consistency. I decide to remove emoji because in this task has labels 1 and 2 to classify pasitive and negative. Beside, the dataset size is large. In order to redundancy reduction, the emoji will also remove. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd015f76",
   "metadata": {},
   "source": [
    "Now the dataset was cleaned we can move to tokenization step.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724dde33",
   "metadata": {},
   "source": [
    "## 4.2 Batch Tokenisztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b93728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#import pandas as pd\n",
    "\n",
    "# Update to a simpler, faster tokenizer for demonstration\n",
    "#def tokenize_and_remove_stopwords(text, stop_words):\n",
    "    # Simple space-based tokenization\n",
    "#    tokens = text.lower().split()\n",
    "#    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "#    return filtered_tokens\n",
    "\n",
    "#def process_chunk(chunk):\n",
    "    # Process each text in the chunk using the tokenizer and stop words removal\n",
    "#    return [tokenize_and_remove_stopwords(text, stop_words) for text in chunk]\n",
    "\n",
    "# Prepare for parallel processing\n",
    "#if __name__ == '__main__':\n",
    "    # Setup multiprocessing pool (adjust processes number as per your system's capability)\n",
    "#    pool = Pool(processes=4)  # Example: 4 parallel processes\n",
    "\n",
    "    # Split data into chunks for processing\n",
    "#    chunks = [train_df['review'][i:i + CHUNK_SIZE] for i in range(0, len(train_df['review']), CHUNK_SIZE)]\n",
    "\n",
    "    # Use pool.map to process chunks in parallel\n",
    "#    processed_chunks = pool.map(process_chunk, chunks)\n",
    "\n",
    "    # Close the pool and wait for the work to finish\n",
    "#    pool.close()\n",
    "#    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "MAX_FEATURES = 1000\n",
    "EMBEDDING_DIM = 120\n",
    "CHUNK_SIZE = 50000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d43173",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Define tokenize_and_remove_stopwords and process_chunk directly in the notebook\n",
    "def tokenize_and_remove_stopwords(text, stop_words):\n",
    "    # Simple space-based tokenization\n",
    "    tokens = text.lower().split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    # Process each text in the chunk using the tokenizer and stop words removal\n",
    "    return [tokenize_and_remove_stopwords(text, stop_words) for text in chunk]\n",
    "\n",
    "# Assuming train_df and other variables are defined as before\n",
    "\n",
    "# Use ThreadPoolExecutor to process chunks\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit tasks to the executor\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in content_chunks]\n",
    "\n",
    "    # Wait for all tasks to complete and collect results\n",
    "    processed_chunks = [future.result() for future in concurrent.futures.as_completed(futures)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65180d56",
   "metadata": {},
   "source": [
    "## 4.3 Preparing data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized text into numerical format and then pad or truncate the sequences to have uniform length\n",
    "\n",
    "# Flatten the list of processed chunks if they are in nested lists\n",
    "processed_texts = [word for chunk in processed_chunks for word in chunk]\n",
    "\n",
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(processed_texts)\n",
    "sequences = tokenizer.texts_to_sequences(processed_texts)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "data = pad_sequences(sequences, maxlen=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca961d",
   "metadata": {},
   "source": [
    "## 4.4 Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca89664",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming `labels` is your array of sentiment labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289c3d3",
   "metadata": {},
   "source": [
    "## 4.5 Build the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97205618",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# neural network (RNN) with LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) layers, or using a Transformer-based model like BERT.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MAX_FEATURES, output_dim=120, input_length=EMBEDDING_DIM))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a283ea",
   "metadata": {},
   "source": [
    "## 4.6 Training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.1, epochs=10, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2de67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64dd230f",
   "metadata": {},
   "source": [
    "## 4.7 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca72947",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b772d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983e479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf450b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c78c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa937af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23daf83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98db23",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Convert tokenized text into numerical format and then pad or truncate the sequences to have uniform length\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize tokenizer with a specified number of words to keep\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(processed_chunks)  # Fit on processed text data\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(processed_chunks)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "data = pad_sequences(sequences, maxlen=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f40124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5346da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bd198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c238758",
   "metadata": {},
   "source": [
    "## Process the dataset in chunks and tokenize each chunk separately \n",
    "\n",
    "BATCH_SIZE = 256\n",
    "MAX_FEATURES = 100\n",
    "EMBEDDING_DIM = 100\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('not')  # Preserve 'not' for sentiment analysis\n",
    "\n",
    "def process_chunk(texts):\n",
    "    # Tokenize and remove stop words in batches\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in texts]  # Added lowercasing here\n",
    "    filtered_texts = [[word for word in text if word not in stop_words] for text in tokenized_texts]\n",
    "    return filtered_texts\n",
    "\n",
    "## Prepare content_chunks using the 'review' column from the dataframe\n",
    "content_chunks = [train_df['review'][i:i + CHUNK_SIZE] for i in range(0, len(train_df['review']), CHUNK_SIZE)]\n",
    "\n",
    "processed_chunks = []\n",
    "for chunk in content_chunks:\n",
    "    processed_chunk = process_chunk(chunk)\n",
    "    processed_chunks.extend(processed_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a00442",
   "metadata": {},
   "source": [
    "## 4.3 Preparaing for Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57cfe2",
   "metadata": {},
   "source": [
    "## 4.2 Tokenisation and pad _sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tokenizer will only consider the top 20,000 most common words in the dataset\n",
    "voc_size = 20000\n",
    "\n",
    "# maximum length of the sequences (lists of tokens) to 100\n",
    "max_length = 100\n",
    "\n",
    "# converting only 20,000 common words into sequences of integer\n",
    "tokenizer = Tokenizer(num_words=voc_size)\n",
    "\n",
    "# mapping from words to integer \n",
    "tokenizer.fit_on_texts(train_df)\n",
    "\n",
    "# word_index is a dictionary mapping words to integer representation. \n",
    "# This can be useful for understanding the tokenization mapping or for further processing.\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# open a file named tokenizer.pkl in write-binary mode ('wb')\n",
    "# The tokenizer object is serialized (converted into a byte stream) using Python's pickle module and \n",
    "# saved to the file. This allows the tokenizer to be saved to disk, making it reusable for \n",
    "# future sessions or other models without the need to re-fit it to the text data.\n",
    "with open('/Users/szuyingpan/Desktop/NLP/CW1/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f036590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the text in the training and testing data\n",
    "#train_sequences = tokenizer.texts_to_sequences(train_df['review'])  \n",
    "#test_sequences = tokenizer.texts_to_sequences(test_df['reviewt'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenizer.texts_to_sequences(train_df['review'])\n",
    "#train = pad_sequences(train_df, maxlen=max_length)\n",
    "test = tokenizer.texts_to_sequences(test_df['review'])\n",
    "#test = pad_sequences(test_df, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab=np.array([1 if i=='2' else 0 for i in train_label])\n",
    "test_lab=np.array([1 if i=='2' else 0 for i in test_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb78d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec210d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba902d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305955ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset is too large to proceed tokenisation step.\n",
    "# Here I tokenise text with spaCy, utilizing its nlp.pipe() method for efficiency and tqdm for progress\n",
    "#!pip install spacy\n",
    "\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"]) # Disabling unnecessary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract documents from the DataFrame column\n",
    "documents = train_df['review'].tolist()\n",
    "\n",
    "# Process documents in batches and use tqdm for progress indication\n",
    "tokenized_docs = []\n",
    "for doc in tqdm(nlp.pipe(documents, batch_size=50), total=len(documents)):\n",
    "    # Extract tokens from each document\n",
    "    tokens = [token.text for token in doc]\n",
    "    tokenized_docs.append(tokens)\n",
    "\n",
    "# If you want to add the tokenized texts back into the DataFrame\n",
    "train_df['tokenized_review'] = tokenized_docs\n",
    "\n",
    "# At this point, `train_df` contains a new column 'tokenized_review' with the tokenized versions of your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed30e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df['tokenized_review'] = train_df['tokenized_review'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save the updated DataFrame to CSV, which now includes the original reviews and their tokenized versions\n",
    "train_df.to_csv(\"tokenized_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab85da2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert tokenized_docs to DataFrame if it makes sense for your dataset\n",
    "df = pd.DataFrame({\"Tokenized_Text\": tokenized_docs})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"tokenized_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e2ea7",
   "metadata": {},
   "source": [
    "### 4.3 Stop words removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the default list of stop words and then remove 'not' from it\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('not')\n",
    "\n",
    "# Now filter text using this customized list\n",
    "filtered_sentence = [word for word in tokenized_sentence if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1cec0",
   "metadata": {},
   "source": [
    "### 4.4 Create n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1d569",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# create uni-gram\n",
    "# We'll use CountVectorizer for this, but just treat it as formal tokenization here\n",
    "vectorizer_unigram = CountVectorizer(analyzer='word')  # default is unigram/1-gram\n",
    "unigram_data = vectorizer_unigram.fit_transform(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d5947",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create bigrams, set ngram_range to (2,2) for bigrams\n",
    "vectorizer_bigram = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "bigram_data = vectorizer_bigram.fit_transform(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65363e51",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create trigrams\n",
    "#vectorizer_trigram = CountVectorizer(analyzer='word', ngram_range=(3,3))\n",
    "#trigram_data = vectorizer_trigram.fit_transform(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence to demonstrate not removing 'not' from the stop words.\"\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the default list of stop words and then remove 'not' from it\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('not')\n",
    "\n",
    "# Tokenize the sample text\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "# Filter out stop words from the tokenized sentence\n",
    "filtered_sentence = [word for word in tokenized_sentence if word not in stop_words]\n",
    "\n",
    "# Let's see the filtered sentence\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f927e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761b63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Unigrams, Bigrams, and Trigrams\n",
    "# Set ngram_range to (1,3) to get unigrams, bigrams, and trigrams\n",
    "vectorizer_trigram = CountVectorizer(analyzer='word', ngram_range=(3,3))\n",
    "trigram_data = vectorizer_trigram.fit_transform(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54aa45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf3de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352e3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab6a0013",
   "metadata": {},
   "source": [
    "## 6. Transforming the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935db86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to evaluate how many n-grams in the summary?\n",
    "\n",
    "# See PPT p.76 skip-gram use ROUGE-S "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d3307",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#def lemmmatization(text, allowed_postages=['NOUN','ADJ','VERB','ADV']):\n",
    "#    nlp = spacy.load('en_core_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa52e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0908657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdceebca",
   "metadata": {},
   "source": [
    "## 7. Model Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5936e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Transformer \n",
    "BERT Embeddings¶\n",
    "BERT is a traditional SOTA transformer architecture published by Google Research which uses bidirectional pretraining . The importance of using BERT is that it has 2 important aspects:\n",
    "\n",
    "Msked Language Model (MLM)\n",
    "Next Sentence Prediction(NSP)\n",
    "The bidirectional pre-training is essentially helpful to be used for any tasks. The Huggingface implementation is helpful for fine-tuning BERT for any language modelling task. The BERT architecture falls under an encoder-decoder(Transformer) model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487efeac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244bee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3962dac",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b25db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# F1,Recall, Precision\n",
    "# Confusion metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb34ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d03ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6efda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e448b7c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 9.Reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d658cd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
